# -*- coding: utf-8 -*-
"""Visual_Attention_Dataset_for_ADHD_ used in Paper.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V1DE-Y54WBI2362NIDBD3FqYmOSSZN4l

Title: Adaptive Deep Self-Supervised Representation based Transfer Learning for Multi-Modal EEG Signal based ADHD Detection

ABSTRACT


Participants were 61 children with ADHD and 60 healthy controls (boys and girls, ages 7-12). The ADHD children were diagnosed by an experienced psychiatrist to DSM-IV criteria, and have taken Ritalin for up to 6 months. None of the children in the control group had a history of psychiatric disorders, epilepsy, or any report of high-risk behaviors.



EEG recording was performed based on 10-20 standard by 19 channels (Fz, Cz, Pz, C3, T3, C4, T4, Fp1, Fp2, F3, F4, F7, F8, P3, P4, T5, T6, O1, O2) at 128 Hz sampling frequency. The A1 and A2 electrodes were the references located on earlobes.

Since one of the deficits in ADHD children is visual attention, the EEG recording protocol was based on visual attention tasks. In the task, a set of pictures of cartoon characters was shown to the children and they were asked to count the characters. The number of characters in each image was randomly selected between 5 and 16, and the size of the pictures was large enough to be easily visible and countable by children. To have a continuous stimulus during the signal recording, each image was displayed immediately and uninterrupted after the child’s response. Thus, the duration of EEG recording throughout this cognitive visual task was dependent on the child’s performance (i.e. response speed).
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from keras.models import Sequential
from keras.layers import LSTM, Dense
import matplotlib.pyplot as plt
## Plotting Libaray
import plotly.express as px
## Pandas Dataframe Library
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import datetime
import json
from imblearn.over_sampling import SMOTE
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
import numpy as np
## Numpy Library
import numpy as np ## used for methamtical compution and handle multi-dimensional arrays and matrices
## Train and Test Split
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
## Normalize
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import  os
from sklearn.model_selection import KFold
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.metrics import r2_score,mean_squared_error, mean_absolute_error
import scipy.io
from scipy.io import loadmat  # this is the SciPy module that loads mat-files

"""# Reading and processing files from matlab"""

directory_pathADHD1 = '/content/drive/MyDrive/XAI for NDD Detection/ADHD_part1'
directory_pathADHD2 = '/content/drive/MyDrive/XAI for NDD Detection/ADHD_part2'
directory_pathH1 = '/content/drive/MyDrive/XAI for NDD Detection/Control_part1'
directory_pathH2 = '/content/drive/MyDrive/XAI for NDD Detection/Control_part2'

dataframesADHD = []
dataframesHealthy = []

for filename in os.listdir(directory_pathADHD1):
    if filename.endswith('.mat'):
        file_path = os.path.join(directory_pathADHD1, filename)
        data = scipy.io.loadmat(file_path)
        key = filename.strip(",.mat")
        key = data[key]
        # print("Key is", key)
        data = pd.DataFrame(key)
        # print("Number of Rows:", data.shape[0])
        # print("Number of Columns:", data.shape[1])
        dataframesADHD.append(data)

for filename in os.listdir(directory_pathADHD2):
    if filename.endswith('.mat'):
        file_path = os.path.join(directory_pathADHD2, filename)
        data = scipy.io.loadmat(file_path)
        key = filename.strip(",.mat")
        key = data[key]
        # print("Key is", key)
        data = pd.DataFrame(key)
        # print("Number of Rows:", data.shape[0])
        # print("Number of Columns:", data.shape[1])
        dataframesADHD.append(data)

for filename in os.listdir(directory_pathH1):
    if filename.endswith('.mat'):
        file_path = os.path.join(directory_pathH1, filename)
        data = scipy.io.loadmat(file_path)
        key = filename.strip(",.mat")
        key = data[key]
        # print("Key is", key)
        data = pd.DataFrame(key)
        # print("Number of Rows:", data.shape[0])
        # print("Number of Columns:", data.shape[1])
        dataframesHealthy.append(data)

for filename in os.listdir(directory_pathH2):
    if filename.endswith('.mat'):
        file_path = os.path.join(directory_pathH2, filename)
        data = scipy.io.loadmat(file_path)
        key = filename.strip(",.mat")
        key = data[key]
        # print("Key is", key)
        data = pd.DataFrame(key)
        # print("Number of Rows:", data.shape[0])
        # print("Number of Columns:", data.shape[1])
        dataframesHealthy.append(data)

if dataframesADHD:
    combined_ADHD = pd.concat(dataframesADHD, ignore_index=True)
else:
    print("No DataFrames available in the list.")

if dataframesHealthy:
    combined_healthy = pd.concat(dataframesHealthy, ignore_index=True)
else:
    print("No DataFrames available in the list.")

dataADHD = pd.DataFrame(combined_ADHD)
dataHealthy = pd.DataFrame(combined_healthy)

print("Number of ADHD Rows:", dataADHD.shape[0])
print("Number of ADHD Columns:", dataADHD.shape[1])
print("Number of Healthy Rows:", dataHealthy.shape[0])
print("Number of Healthy Columns:", dataHealthy.shape[1])

dataADHD.head()

dataHealthy.head()

dataADHD['label'] = 'ADHD'

dataADHD.head()

dataHealthy['label'] = 'Healthy'

dataHealthy.head()

data = pd.concat([dataADHD, dataHealthy], ignore_index=True)

columns=['Fp1','Fp2','F3','F4','C3','C4','P3','P4','O1','O2','F7','F8','T7','T8','P7','P8','Fz','Cz','Pz','label']

data.columns=columns

data.shape

data = data.sample(frac=1).reset_index(drop=True)

data.to_csv('/content/drive/MyDrive/Visual Attnetion/Visual Attention Dataset for ADHD.csv')

"""# Read the final Dataset"""

# data=pd.read_csv('/content/drive/MyDrive/Visual Attnetion/Visual Attention Dataset for ADHD-undersample.csv')
data=pd.read_csv('C:/Users/admin/Downloads/Visual Attention Dataset for ADHD (1).csv')

## Dimensionlaity of the Model
print("Number of Rows:", data.shape[0])
print("Number of Columns:", data.shape[1])

data['label'].unique()

label_counts = data['label'].value_counts()

# Display the counts for each label
print("Number of instances for each label:")
print(label_counts)

# Identify categorical columns
from sklearn.preprocessing import LabelEncoder
categorical_columns = data.select_dtypes(include=['object']).columns

# Initialize the LabelEncoder
label_encoder = LabelEncoder()

# Apply label encoding to each categorical column
for column in categorical_columns:
    data[column] = label_encoder.fit_transform(data[column])

data.head()

data=data.drop('Unnamed: 0', axis=1)

# # Function to remove outliers using Z-score
# def remove_outliers_zscore(data, threshold=4.5):
#     z_scores = (data - data.mean()) / data.std()
#     return data[(z_scores.abs() < threshold).all(axis=1)]

# features = data.drop('label', axis=1)
# target = data['label']
# features_no_outliers = remove_outliers_zscore(features)

# # Split the data into features (X) and target (y)
# X = features_no_outliers
# y = target[features_no_outliers.index]

X = data.drop(['label'], axis=1).reset_index(drop=True)
y = data['label'].reset_index(drop=True)

## Dimensionlaity of the Model
print("Number of Rows:", X.shape[0])
print("Number of Columns:", X.shape[1])

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Instantiate SMOTE
smote = SMOTE(random_state=42)

# Apply SMOTE to the training set
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

# Check the class distribution before and after SMOTE
print("Class distribution before SMOTE:", dict(zip(*np.unique(y_train, return_counts=True))))
print("Class distribution after SMOTE:", dict(zip(*np.unique(y_resampled, return_counts=True))))

X_train=X_resampled
y_train=y_resampled

# Standardize features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""

# Random forest"""

#feature selection
from sklearn.ensemble import RandomForestClassifier
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense

# # Reshape data for CNN input
# X_train_3d = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
# X_test_3d = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)

# # Build CNN for feature extraction
# cnn_model = Sequential()
# cnn_model.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(X_train_3d.shape[1], 1)))
# cnn_model.add(MaxPooling1D(pool_size=2))
# cnn_model.add(Flatten())

# # Extract features using the CNN
# X_train_features = cnn_model.predict(X_train_3d)
# X_test_features = cnn_model.predict(X_test_3d)

# # Reshape the features for Random Forest input
# X_train_features = X_train_features.reshape(X_train_features.shape[0], -1)
# X_test_features = X_test_features.reshape(X_test_features.shape[0], -1)

# Standardize the features
scaler = StandardScaler()
X_train_features = scaler.fit_transform(X_train)
X_test_features = scaler.transform(X_test)

# Train Random Forest
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train_features, y_train)

# Evaluate the model
accuracy = rf_model.score(X_test_features, y_test)
print(f"Accuracy on the test set: {accuracy:.2f}")

from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc
import matplotlib.pyplot as plt

# Predictions
y_pred_rf = rf_model.predict(X_test_features)

# Generate classification report
target_names = ['ADHD', 'Normal']  # Replace with your class names
classification_rep = classification_report(y_test, y_pred_rf, target_names=target_names)
print("Classification Report:\n", classification_rep)

# Generate confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred_rf)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()


# Classification Report
report_rf = classification_report(y_test, y_pred_rf, target_names=target_names)
print(f"Classification Report for Random Forest:\n{report_rf}\n")

# ROC curve
y_prob_rf = rf_model.predict_proba(X_test_features)
fpr_rf, tpr_rf, _ = roc_curve(y_test, y_prob_rf[:, 1])
roc_auc_rf = auc(fpr_rf, tpr_rf)
plt.figure()
plt.plot(fpr_rf, tpr_rf, color='darkorange', lw=2, label=f"ROC curve (AUC = {roc_auc_rf:.2f})")
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title("ROC Curve for Random Forest")
plt.legend(loc="lower right")
plt.show()

"""# LSTM and GRU layers representation learning"""

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, LSTM, GRU, Input, SimpleRNN, TimeDistributed, Flatten, concatenate
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
import numpy as np

#LSTM and GRU layers representation learning
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, LSTM, Input, TimeDistributed, Flatten, concatenate
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, LSTM, GRU, Input, SimpleRNN, TimeDistributed, Flatten, concatenate
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
import numpy as np
# Reshape data for CNN input
X_train_3d = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
X_test_3d = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)

# Define the self-supervised learning model
input_layer = Input(shape=(X_train_3d.shape[1], 1))

# Multiple LSTM and GRU layers for representation learning
lstm_1 = LSTM(64, return_sequences=True)(input_layer)
gru_1 = GRU(64, return_sequences=True)(input_layer)
lstm_gru_concat = concatenate([lstm_1, gru_1])
lstm_2 = LSTM(32, return_sequences=True)(lstm_gru_concat)
gru_2 = GRU(32, return_sequences=True)(lstm_gru_concat)

# Fully connected layers for self-supervised learning
fc_lstm = TimeDistributed(Dense(16))(lstm_2)
fc_gru = TimeDistributed(Dense(16))(gru_2)

# Flatten the output and concatenate for transfer learning
flatten_lstm = Flatten()(fc_lstm)
flatten_gru = Flatten()(fc_gru)
combined_representations = concatenate([flatten_lstm, flatten_gru])

# Fully connected layers for transfer learning (binary classification)
fc_transfer = Dense(64, activation='relu')(combined_representations)
output = Dense(1, activation='sigmoid')(fc_transfer)  # Binary classification

model = Model(inputs=input_layer, outputs=output)

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Commented out IPython magic to ensure Python compatibility.
# %%time
# history=model.fit(X_train_3d, y_train, epochs=40, batch_size=32, validation_data=(X_test_3d, y_test))

model.save('/content/drive/MyDrive/Visual Attnetion/main-modelbetteraccuracy.h5')

from tensorflow.keras.utils import plot_model
# Visualize the model architecture
plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)

model.summary()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
from sklearn.metrics import roc_curve, auc

# Evaluate the model
from sklearn.metrics import accuracy_score
y_pred = (model.predict(X_test_3d) > 0.5).astype(int)

accuracy = accuracy_score(y_test, y_pred)
print(f"Test Accuracy: {accuracy:.4f}")

# Assuming you have the history object from model training

# Plot training and validation accuracy
plt.figure(figsize=(10, 5))
plt.plot(history.history['accuracy'], label='Train Accuracy',marker='.')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy',marker='.')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Plot training and validation loss
plt.figure(figsize=(10, 5))
plt.plot(history.history['loss'], label='Train Loss',marker='.')
plt.plot(history.history['val_loss'], label='Validation Loss',marker='.')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Generate classification report
target_names = ['ADHD', 'Normal']  # Replace with your class names
classification_rep = classification_report(y_test, y_pred, target_names=target_names)
print("Classification Report:\n", classification_rep)

# Generate confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()


# Make predictions on the test set
y_probs = model.predict(X_test_3d)

# Compute ROC curve for class 1
fpr, tpr, _ = roc_curve(y_test, y_probs)
roc_auc = auc(fpr, tpr)

# Plot ROC curve
plt.figure(figsize=(8, 8))
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc))
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.show()

"""# Early Stopping with LSTM and GRU based representatino Learning"""

#LSTM and GRU layers representation learning
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, LSTM, Input, TimeDistributed, Flatten, concatenate
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, LSTM, GRU, Input, SimpleRNN, TimeDistributed, Flatten, concatenate
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
import numpy as np
# Reshape data for CNN input
X_train_3d = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
X_test_3d = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)

# Define the self-supervised learning model
input_layer = Input(shape=(X_train_3d.shape[1], 1))

# Multiple LSTM and GRU layers for representation learning
lstm_1 = LSTM(32, return_sequences=True)(input_layer)
gru_1 = GRU(32, return_sequences=True)(input_layer)
lstm_gru_concat = concatenate([lstm_1, gru_1])
lstm_2 = LSTM(64, return_sequences=True)(lstm_gru_concat)
gru_2 = GRU(64, return_sequences=True)(lstm_gru_concat)

# Fully connected layers for self-supervised learning
fc_lstm = TimeDistributed(Dense(16))(lstm_2)
fc_gru = TimeDistributed(Dense(16))(gru_2)

# Flatten the output and concatenate for transfer learning
flatten_lstm = Flatten()(fc_lstm)
flatten_gru = Flatten()(fc_gru)
combined_representations = concatenate([flatten_lstm, flatten_gru])

# Fully connected layers for transfer learning (binary classification)
fc_transfer = Dense(32, activation='relu')(combined_representations)
output = Dense(1, activation='sigmoid')(fc_transfer)  # Binary classification

model = Model(inputs=input_layer, outputs=output)

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from tensorflow.keras.callbacks import EarlyStopping
# # Define early stopping callback
# early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
# 
# # Train the model with early stopping
# earlyhistory = model.fit(X_train_3d, y_train, epochs=40, batch_size=32, validation_split=0.2, callbacks=[early_stopping])

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
from sklearn.metrics import roc_curve, auc

# Evaluate the model
from sklearn.metrics import accuracy_score
y_pred = (model.predict(X_test_3d) > 0.5).astype(int)

accuracy = accuracy_score(y_test, y_pred)
print(f"Test Accuracy: {accuracy:.4f}")

# Assuming you have the history object from model training

# Plot training and validation accuracy
plt.figure(figsize=(10, 5))
plt.plot(earlyhistory.history['accuracy'], label='Train Accuracy',marker='.')
plt.plot(earlyhistory.history['val_accuracy'], label='Validation Accuracy',marker='.')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Plot training and validation loss
plt.figure(figsize=(10, 5))
plt.plot(earlyhistory.history['loss'], label='Train Loss',marker='.')
plt.plot(earlyhistory.history['val_loss'], label='Validation Loss',marker='.')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Generate classification report
target_names = ['ADHD', 'Normal']  # Replace with your class names
classification_rep = classification_report(y_test, y_pred, target_names=target_names)
print("Classification Report:\n", classification_rep)

# Generate confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()


# Make predictions on the test set
y_probs = model.predict(X_test_3d)

# Compute ROC curve for class 1
fpr, tpr, _ = roc_curve(y_test, y_probs)
roc_auc = auc(fpr, tpr)

# Plot ROC curve
plt.figure(figsize=(8, 8))
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc))
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.show()

"""# LSTM fewer Layers representation layer"""

# Assuming X_train_df and X_test_df are DataFrames
# X_train = X_train.values  # Extract the values from the DataFrame
# X_test = X_test.values

# Reshape data for LSTM input (3D)
X_train_3d = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
X_test_3d = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)


# Define the model for self-supervised learning (pretext task)
input_layer = Input(shape=(X.shape[1], 1))
lstm = LSTM(32)(input_layer)
output_pretext = Dense(X.shape[1], activation='linear')(lstm)  # Predict the future in the sequence

model_pretext = Model(inputs=input_layer, outputs=output_pretext)

# Compile the pretext task model
model_pretext.compile(optimizer='adam', loss='mean_squared_error')

# Train the pretext task model (self-supervised learning)
model_pretext.fit(X_train_3d, X_train, epochs=40, batch_size=32, validation_data=(X_test_3d, X_test))

model_pretext.summary()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Use the learned representations for downstream binary classification
# representation_layer = Model(inputs=input_layer, outputs=lstm)
# 
# # Freeze the representation layer
# representation_layer.trainable = False
# 
# # Define the binary classification model using learned representations
# input_representation = Input(shape=(X.shape[1], 1))
# representation = representation_layer(input_representation)
# fc = Dense(32, activation='relu')(representation)
# output_classification = Dense(1, activation='sigmoid')(fc)  # Binary classification
# 
# model2 = Model(inputs=input_representation, outputs=output_classification)
# 
# # Compile the binary classification model using learned representations
# model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
# # Train the binary classification model using learned representations
# history=model2.fit(X_train_3d, y_train, epochs=10, batch_size=64, validation_data=(X_test_3d, y_test))

model2.summary()

# Evaluate the model
from sklearn.metrics import accuracy_score
y_pred = (model2.predict(X_test_3d) > 0.5).astype(int)
accuracy = accuracy_score(y_test, y_pred)
print(f"Test Accuracy: {accuracy:.4f}")

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns

# Plot training and validation accuracy
plt.figure(figsize=(10, 5))
plt.plot(history.history['accuracy'], label='Train Accuracy',marker='.')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy',marker='.')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Plot training and validation loss
plt.figure(figsize=(10, 5))
plt.plot(history.history['loss'], label='Train Loss',marker='.')
plt.plot(history.history['val_loss'], label='Validation Loss',marker='.')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Generate classification report
target_names = ['ADHD', 'Normal']  # Replace with your class names
classification_rep = classification_report(y_test, y_pred, target_names=target_names)
print("Classification Report:\n", classification_rep)

# Generate confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

model.save('/content/drive/MyDrive/Visual Attnetion/my_model2.h5')



from tensorflow import keras
import tensorflow as tf

class FeatureAttentionLayer(keras.layers.Layer):
    def __init__(self, units):
        super(FeatureAttentionLayer, self).__init__()
        self.units = units

    def build(self, input_shape):
        self.W_q = self.add_weight(name="W_q", shape=(input_shape[-1], self.units), initializer="glorot_uniform")
        self.W_k = self.add_weight(name="W_k", shape=(input_shape[-1], self.units), initializer="glorot_uniform")
        self.W_v = self.add_weight(name="W_v", shape=(input_shape[-1], self.units), initializer="glorot_uniform")

    def call(self, inputs):
        query = tf.matmul(inputs, self.W_q)
        key = tf.matmul(inputs, self.W_k)
        value = tf.matmul(inputs, self.W_v)

        attention_scores = tf.matmul(query, key, transpose_b=True)
        attention_scores = tf.nn.softmax(attention_scores)

        output = tf.matmul(attention_scores, value)

        return output

# Build the model
model = keras.Sequential([
    FeatureAttentionLayer(units=60),
    keras.layers.Dense(100, activation='relu'),
    keras.layers.Dense(50, activation='relu'),
    keras.layers.Dense(1, activation='sigmoid')
])

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))
# Evaluate the model
test_loss, test_acc = model.evaluate(X_test, y_test)
print(f'Test accuracy: {test_acc * 100:.2f}%')





